---
title: "Foundations + Context"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dimension Reduction

  Dimension reduction is a common technique used for working with large data sets with the goal moving data from a high-dimensional feature space to a low-dimensional feature space while retaining key features of the original data. Approaches to dimension reduction can generally broken into one of two categories; feature selection and feature extraction. Feature selection techniques remove unneeded features from the data. For example, feature selection before building a regression model will often include remove predictors with low correlation to the response variable and predictors with high correlation.
  Feature extraction techniques will create a new set of features that capture relevant information about the original data. For example, a feature extraction technique may create linear combinations of our original predictors. It is important to note that feature selection will return a subset of variables from the original data set while feature extraction will return a set of new variables.
  

## Curse of Dimensionality

  At first, dimension reduction may seem like a counter intuitive approach to take when working with data. It seems that the more features we have, the more we should be able to learn about our data. However, when working with large datasets, the curse of dimensionality comes into play. The curse of dimensionality is a term used to refer to a variety of issues that working with high dimensional data presents. As the dimension of feature space increases, data tends to become increasingly sparse making trends in data more difficult to recognize. One way to think of how this problem manifests is to consider how the ratio of the volume of ball and the hypercube containing the ball changes as the dimension $d$ increases.

  The volume of a ball, $B$, of radius $r$ in $n$ dimensions is given by
$$
\frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+1)}r^n
$$
The volume of the hypercube,$C$, containing this ball (with sides of length $2r$) is given by
$$
(2r)^n
$$

Taking the ratio we have
$$
\frac{\text{Vol}(B)}{\text{Vol}(C)} = \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+1)\cdot2^n} \text{ so } \text{lim}_{n \to \infty}\frac{\text{Vol}(B)}{\text{Vol}(C)}=0
$$

Consider the case where $r=1$, 

```{r}
library(gt)

ratio_df <- data.frame(n = c(1,2,5,10,25,100), Ratio = c('1', '0.7854', '0.1645', '2.49*e-3', '2.854e-11', '1.868e-70'))

gt(ratio_df)
```


Thus, we can expect the number of points within distance 1 of a point will decrease as the dimension of data increases.

Another problem that arises from high dimensional data is computational time. The computation complexity of fitting a linear regression model to an $n\times p$ matrix of predictors is $O(np^2+p^3)$ (many programs will not perform the full matrix inversion so the complexity is usually smaller in practice however the general principle of computational complexity increasing holds). This will quickly balloon for large $p$. More troubling in high dimensions is that for $p$ predictors, there are $2^p$ possible combinations of predictors making techniques such as $\textit{Best Subset Selection}$ computationally prohibitive.

## Principal Component Analysis

  Principal component analysis(PCA) is a commonly used unsupervised learning technique. PCA works by finding the principle components of a dataset, these can be thought of as the direction along which the data varies the most in the feature space. For those of you with a background in linear algebra, the principal components will be the eigenvectors of the covariance matrix in order of the norm of the eigenvalues.
    PCA is one of the most commonly used dimension reduction techniques as it is often able to capture a large amount of the variation in a data set using only a few features. PCA returns a series of principal components, these can be thought of as vectors in the original feature space. Principal components will be orthogonal to each other so the variance explained by each component will be unique. Given an $n \times p$ data set, $n$ principal components can be created where each principal component is a vector of length $p$.
    The algorithm for finding the first principal component of a standardized $n \times p$ data matrix $\textbf{X}$ is as follows:
$$
\textbf{w}_1 = \text{argmax}_{||\textbf{w}||=1} \Big\{ \textbf{w}^T \textbf{X}^T\textbf{X} \textbf{w} \Big\}
$$
Note that $\textbf{X}^T\textbf{X}$ is the covariance matrix for $\textbf{X}$. For all following components, we will find the matrix $\hat{\textbf{X}}_k$ such that all variance explained by the first $k-1$ principal components is removed. This is done by finding
$$
\hat{\textbf{X}}_k = \textbf{X} - \sum_{j = 1}^{k-1} \textbf{X} \textbf{w}_j \textbf{w}_j^T
$$
We then use the same equation used to find $\textbf{w}_1$ to find the $k$th principle component, replacing $\textbf{X}$ with $\hat{\textbf{X}}_k$.
  Note that principal components can often be used for latent variables.
  
## Linear Regression

  Linear regression is perhaps the most commonly used regression method given its simplicity and that it often yields fairly well performing models. Many other regression techniques use linear regression as a foundation. Linear regression works to find the line in $n$ space that minimizes the Mean Squared Error (the squared distance between the line and observed data). The MSE is used in order to ensure that there is a unique answer. In linear regression we are looking for terms $\beta_0,...,\beta_n$ such that
$$
\sum_{i=1}^p (y_i - \hat{y_i})^2
$$
is minimized where
$$
\hat{y} = \beta_0 + \beta_1 \cdot x_1 + ... + \beta_n x_n
$$
Note that we are working with a $n+1 \times p$ data frame with $n$ predictors, 1 response variable, and $p$ observations. \bigskip

We can find the vector of regression coefficients $\boldsymbol{\beta}$ using linear algebra by solving the equation 
$$
\boldsymbol{\beta} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X} \textbf{Y}
$$
where $\textbf{X}$ is an $n \times p$ matrix of predictors and $\textbf{Y}$ is a $1 \times p$ matrix of the response variable.

Alternatively, we can find $\beta_i$ as follows,
$$
\beta_i = \frac{\langle \textbf{x}_i, \textbf{y} \rangle}{\langle \textbf{x}_i, \textbf{x}_i \rangle}
$$


## Principal Component Regression

Principal Component Regression (PCR) is a regression technique that uses principal components as predictors. To perform PCR, one first finds the desired number of principal components and then performs linear regression using the selected principal components as predictors. Letting $\textbf{w}_1, \textbf{w}_2,...,\textbf{w}_k$ be the first $k$ principal components of our predictors $\textbf{X}$. This will take the form 
$$
\textbf{y} = \theta_0 + \sum_{i = 1}^k \theta_i \textbf{w}_i
$$
where
$$
\theta_i = \frac{\langle \textbf{w}_i, \textbf{y} \rangle}{\langle \textbf{w}_i, \textbf{w}_i \rangle}
$$
For high dimensional data, PCR can avoid problems of over fitting that occur with a large number of predictors. Furthermore, it can provide more interpretable models if the principal components can be linked to latent variables. PCR does have some drawbacks, since principal components only depend on variance within predictors, PCR can miss predictors that contribute little to variance among predictors but strongly explain variance among the response variable.


## Partial Least Square

Partial Least Square (PLS) is a regression technique with similarities to PCR. Unlike PCR, PLS considers the covariance matrix instead of the variance matrix. By using the covariance matrix, we ensure that the latent variables of each order will have the most possible correlation with the response variable. For example, the first principle component may only be able to explain most of the variance in the predictors but very little in the response whereas the first component in PLS will explain as much of the variance in the response as possible. This allows it to make more accurate predictions than PCR when using models of the same complexity.

Like PCA, PLS also require all variables to be standardized with $\mu = 0$ and $\sigma^2 = 1$. We then find the $m$th latent variable $\textbf{z}_m$ using the following formula
$$
\textbf{z}_m = \sum_{i = 1}^p \hat{\varphi}_{mi} \textbf{x}_j^{(m-1)} 
$$
where $\hat{\varphi}_{mi} = \langle \textbf{x}_j^{(m-1)},\textbf{y} \rangle$. \bigskip

We then find the regression coefficient $\hat{\theta}_m$ for $\textbf{z}_m$ as follows
$$
\hat{\theta}_m = \frac{\langle \textbf{z}_m , \textbf{y} \rangle}{\langle \textbf{z}_m , \textbf{z}_m \rangle}
$$
Note that this is identical to the formula used in classic linear regression and PCR at this step. \bigskip



## $L_1$ Sparsification

$L_1$ sparsification is a common shrinkage method used to avoid over fitting models. The $L_1$ penalty is most commonly used in the LASSO model (it is sometimes referred to as the LASSO penalty). The $L_1$ penalty is added to a model by adding a term of $\lambda \sum_{i = 1}^p | \beta_i|$ where $\lambda$ is a parameter for the model. Note that as $\lambda \to 0$ the model will become unchanged by the penalty and as $\lambda \to \infty$, $\beta_i = 0$ for all $i$. If $\lambda$ is sufficiently large, we will begin to see some $\beta_i = 0$ which makes the $L_1$ norm special among commonly used norms.

!Add that diagram here!



##  Sections to add
  PCA +Linear Regression?
  PCR
  PLS
  $L^1$ Sparsification
  PLS1 and PLS2 + algorithms
  Multivariate regression
  
  Write about data
  
  
## Why Partial Least Squares?

Regression models that use latent variables can help with the curse of dimensionality when building models for high dimensional data. When $n < p$ (the number of predictors is larger than the sample size) or $n \approx p$ many traditional regression methods will lose predictive power as they become increasingly susceptible to noise in the data. For example, using single variate least squares regression when $n < p$ is extremely likely to over fit the data. Furthermore, solutions that minimize the squared error will no longer be unique.

Models that use latent variables will be able to avoid this problem by projecting the data into a lower dimensional feature space so that $n >p$. Since we project from the full feature space less information about the data will be lost than by simply performing variable selection.

Sparse partial least squares further mitigates problems of over fitting the data as it...



# Literature Review


Partial Least Squares was first proposed by Herman Wold in 1975 under the name Non-linear Iterative Partial Least Squares (NIPALS) in order to model complex datasets. Later models built using the framework of NIPALS would more simply be referred to by the name Partial Least Squares(PLS). Wold would later suggest that the model be referred to as Projection onto Latent Structures as he found this name to more accurately describe the model. Parameters for the model are found as the result of bivariate least squares linear regression using a latent variable believed to describe the underlying relation between predictor and response variables. These latent variables are then iterated over to estimate model parameters.

PLS and variants have shown to be some of the best performing models for working with high dimensional data. This has lead to its widespread use in fields such as chemometrics, genomics, spectrometry, as well as others that work with data where the number of predictors is close to or greater than the number of observations. 

PLS models generally follow a five step process as outlined by Lorenzo et al.

a. Estimate the covariance matrix between the predictor ($\textbf{X}$) and response ($\textbf{Y}$) variables.

b. Estimate the singular-space associated with the largest singular-value of the previous matrix.

c. Project the covariate and response parts on the previously defined subspace.

d. Estimate the linear regression matrix of the response part on the covariate part in the subspace.

e. Remove the information carried by the current subspace from the covariate and response parts

Although not a problem unique to PLS, model consistency is shown to decrease as the ratio of predictors to observations increases. This is due to models tendency to overfit the data as they are unable to distinguish between noise and the underlying relationships of variables. In order to improve on these problems of overfitting, $L_1$ penalization of regression coefficients was introduced by Tibshirani in 1995 with the Lasso method. The Least Absolute Shrinkage and Selection Operator(Lasso) model is based on Ordinary Least Squares Regression(OLS) and introduces a penalty term to enforce sparsity among predictors.

The basic idea of penalization is to discourage models from including larger coefficients for terms that only explain a small amount of variance. This prevents models from overfitting data as the model will give less weight to terms that only explain noise in the data when trained on a sufficiently large sample. Unlike similar models with a penalty term such as Ridge Regression, Lasso also performs variable selection.

The Lasso model contains one parameter, $\lambda$, which takes positive real numbers as values and decide how heavily the model will be penalized. When 0 is chosen, the model is identical to partial least squares. As $\lambda$ increases, the model will become increasingly sparse. When $\lambda \to \infty$, the model will become $\hat{y} = \beta_0 = \bar{X}$ as all coefficients other than $\beta_0$ are penalized. Unlike other shrinkage methods, Lasso also performs variable selection as the $L_1$ norm will send coefficients to 0 for sufficiently large $\lambda$. This means that Lasso tends to select simpler more interpretable models than 
similar techniques. Parameter selection is usually performed through cross validation, a method originally proposed by Tibshirani.

Sparse Partial Least Squares(SPLS) is a PLS method that uses the $L_1$-norm to achieve a sparser covariance matrix. Originally proposed in 2007/2008 by either Chun and Keles or Le Kao, Rossow, Robert-Grani√©, and Besse. By performing variable shrinkage and selection with the covariance matrix, we are trying to not project onto a latent structure that is overly determined by noise in our observed data.


## Junk

1. Data-Driven Sparse Partial Least Squares - Hadrien Lorenzo

  Proposes ddsPLS model, subspace selection is done based on variable selection. Preliminary results suggest the model performs better both with variable selection and overall performance. Unlike many similar models, ddsPLS performs parameter selection through bootstrapping instead of cross validation.

2. Can We Beat Over-fitting - Olivier Cloarec

  Explores problem of over fitting and suggest modified NIPALS PLS algorithm to offer improvement.

3. Globally Sparse PLS Regression - Liu Tzu-Yu

  

4. On the over estimation of Random Forest's out-of-bag error - Silke Janitza
5. Some aspects of response variable selection and estimation in multivariate linear regression - Jianhua Hu

