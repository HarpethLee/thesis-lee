---
title: "Results"
author: "Harpeth Lee"
date: "4/2/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(latex2exp)
library(ggpmisc)
library(gridExtra)
library(dplyr)
library(gt)
```


# Results

## Data Simulation

Our data simulations are done based on the latent variable model that PLS methods assume. We generate $\textbf{X}$ and $\textbf{Y}$ using the following equations, $\textbf{X} = \boldsymbol{\phi} \textbf{A} + \boldsymbol{\epsilon}_X$ and $\textbf{Y} = \boldsymbol{\phi} \textbf{D} + \boldsymbol{\epsilon}_Y$. The $\textbf{A}$ and $\textbf{D}$ matrices are fixed, providing most of the structure seen in $\textbf{X}$ and $\textbf{Y}$. $\boldsymbol{\phi}$, $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$ are randomly generated. The method by which $\boldsymbol{\phi}$ is generated is less important as we want to approximate $\boldsymbol{\phi}$ with our final model. $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$ are sampled from a normal distribution.

Although $\boldsymbol{\phi}$ is generated using a similar method as $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$, it is important to note that $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ are noise while $\boldsymbol{\phi}$ is not. Ideally, our models will be able to identify $\boldsymbol{\phi}$ while ignoring $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$. $\boldsymbol{\phi}$ is generated sampling from a multivariate normal distribution with uncorrelated samples(using the `mvrnorm` function in the `MASS` package in order to make sure columns are uncorrelated). $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ are generated by samples from random distributions (using `rnorm`) as this will cause some random correlation in the noise. This random correlation is included as we want to test model's ability to distinguish true correlation from random correlation.

When generating the data, there are four variables related to the dimensionality of the matrices which must be chosen, $n$, $p$, $q$, and $R$. $n$ is the number of observations generated, $p$ is the number of predictors, and $q$ is the number of response variables. Of these variables, $R$ is the only one of these variables that is not clear from the generated data. $R$ is the size of the dimension of $\textbf{A}$,$\textbf{D}$, and $\boldsymbol{\phi}$ that is lost when matrix multiplication is performed. Since we want $\textbf{X}$ to be of dimension $n \times p$ and $\textbf{Y}$ to be of dimension $n \times q$, $\boldsymbol{\phi}$ will be of dimension $n \times R$, $\textbf{A}$ will be of dimension $R \times p$, and $\textbf{D}$ will be of dimension $R \times q$. Although it may seem like we will want our final model to have $R$ total components, we will want $K$ to be the number of eigenvectors of the variance matrix of $\textbf{X}$ that have non-null projections onto the covariance matrix of $\textbf{X}$ and $\textbf{Y}$. [Helland and Almoy]

There were a number of methods used to generate these matrices. $\boldsymbol{\phi}$ was always generated from uncorrelated normal distributions. Two methods were used to generate $\textbf{A}$, the "simple" method and the "complex" method. In the simple method, 
$$
\textbf{A} =
\begin{pmatrix}
\textbf{1}_{\frac{3}{5}R \times 5} & \textbf{0}_{\frac{3}{5}R \times 10} & \textbf{0}_{\frac{3}{5}R \times (p-15)}  \\
\textbf{0}_{\frac{2}{5}R \times 5} & \textbf{1}_{\frac{2}{5}R \times 10} & \textbf{0}_{\frac{2}{5}R \times (p-15)}
\end{pmatrix}
$$
Note that $p \geq 15$ in order for this method to work. The dimensions of $\frac{3}{5}R$ and $\frac{2}{5}R$ are not exact, we round and then take the difference in order to make sure values are integers that add up to R.

Using the "complex" method,
$$
\textbf{A} = 
\begin{pmatrix}
\textbf{1}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 2} & \textbf{0}_{3 \times (p-23)} \\
\textbf{0}_{2 \times 5} & \textbf{1}_{2 \times 10} & \textbf{0}_{2 \times 5} & \textbf{0}_{2 \times 1} & \textbf{0}_{2 \times 2} & \textbf{0}_{2 \times (p-23)} \\
\textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{1}_{3 \times 5} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 2} & \textbf{0}_{3 \times (p-23)} \\
\textbf{0}_{2 \times 5} & \textbf{0}_{2 \times 10} & \textbf{0}_{2 \times 5} & \textbf{1}_{2 \times 1} & \textbf{0}_{2 \times 2} & \textbf{0}_{2 \times (p-23)}\\
\textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 1}  & \textbf{1}_{3 \times 2} & \textbf{0}_{3 \times (p-23)}
\end{pmatrix}
$$
Note that $p \geq 23$ in order for this method to work. Furthermore, $R=13$ whenever this method is used.

A wider number of methods were used to generate the $\textbf{D}$ matrix. We will call these methods "basic", "diagonal", "simple", "2-block", and "4-block". In the basic method, 
$$
\textbf{D} = \textbf{1}_{R \times q}
$$
Using the "diagonal" method,
$$
\textbf{D} = \textbf{I}_{R \times q}
$$
Using the "simple" method
$$
\textbf{D} = 
\begin{pmatrix}
\textbf{1}_{R \times 1} & \textbf{0}_{R \times (q-1)}
\end{pmatrix}
$$
Using the "2-block" method
$$
\textbf{D} = 
\begin{pmatrix}
\textbf{1}_{\frac{3}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{3}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{3}{5} \times \frac{1}{2}q} \\
\textbf{0}_{\frac{2}{5}R \times \frac{1}{4}q} & \textbf{1}_{\frac{2}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{2}{5} \times \frac{1}{2}q}
\end{pmatrix}
$$
Note the two blocks of 1s hence the name "2-block" method.

Using the "4-block" method
$$
\textbf{D} =
\begin{pmatrix}
\textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
 \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times (q-4)} & 
\end{pmatrix}
$$
Note that $R=13$ at all times when the "4-block" method is used.

The $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ matrices are generated from samples from a standard normal distribution and then scaled by a factor $\omega$, called the noise weight. $\boldsymbol{\epsilon}_X$ is of dimension $n \times p$ and $\boldsymbol{\epsilon}_Y$ is of dimension $n \times q$. Since the noise is randomly generated we expect to see some correlation occur both between predictors and between the predictors and the responses.


## Noise

When examining model performance, it is important to consider how it performs under conditions where the amount of noise varies. The following graphs show the relationship between $\omega$ and the RMSE for the ddsPLS, sPLS, and PLS methods. Note the extremely linear relation displayed in all three.

```{r, echo=FALSE}
samp_test_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/ddspls_noise_weight_1.csv")
samp_test_spls <-read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/spls_noise_weight_1.csv")
colnames(samp_test_spls) <- c("X", "noise_weight", "ncomp", "RMSE", "R2", "Q2", "R2.Q2")
samp_test_pls <-read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/pls_noise_weight_1.csv")

samp_test_ddspls  <- samp_test_ddspls %>%
  mutate(model = "ddsPLS")
samp_test_spls <- samp_test_spls %>%
  mutate(model = "sPLS")
samp_test_pls <- samp_test_pls %>%
  mutate(model = "PLS")

samp_test <- rbind(samp_test_ddspls, samp_test_spls, samp_test_pls)

samp_test$model <- factor(samp_test$model,
    levels = c("ddsPLS", "sPLS", "PLS"),
    ordered = TRUE)
```


```{r, fig.align='center', echo=FALSE, out.width="40%", cache=TRUE}
ddspls_plot <- ggplot(samp_test_ddspls,
                      aes(x = noise_weight, y = RMSE)) +
  geom_point(color = "turquoise3") +
  geom_smooth(method = "lm",
              size = 0.25,
              se= FALSE,
              formula = y ~ x,
              color = "black") +
   stat_poly_eq(formula = y~x, 
                aes(label = paste(..eq.label..)), 
                parse = TRUE) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       title = "ddsPLS", 
       caption = unname(TeX("Relation between $\\omega$ and the test set RMSE for the ddsPLS model. $$n=100, p=100, q=5$$"))) +
  theme_minimal()
```


```{r, echo=FALSE, cache=TRUE}
samp_test %>%
  ggplot(aes(x = noise_weight, y = RMSE, color = model)) +
  geom_point(alpha = 0.75) +
  geom_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              size = 0.5) +
   stat_poly_eq(formula = y~x, 
                aes(label = paste(..eq.label..)), 
                parse = TRUE) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")), 
       caption = unname(TeX("Relation between $\\omega$ and the test set RMSE for the PLS model. $$n=100, p=100, q=5$$")),
       color = "Model") +
  theme_minimal()
```


Note the similar slopes of the line of best fit for all model types. Ultimately, these plots suggest that there is not a large difference in performance between model types as the amount of noise is increased. We should be careful about drawing any conclusions from results at higher values of $\omega$ as these models are usually just performing mean estimation and are not as dependent on the model used.


Now lets compare $R^2$ and $Q^2$ as $\omega$ changes.

```{r, echo=FALSE, cache=TRUE}
R2_plot <- ggplot(samp_test, aes(x = noise_weight, y = R2, color = model)) +
  geom_point(alpha = 0.6) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = unname(TeX("$R^2$"))) +
  theme_minimal() +
  theme(legend.position = "none")

Q2_plot <- ggplot(samp_test, aes(x = noise_weight, y = Q2, color = model)) +
  geom_point(alpha = 0.6) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = unname(TeX("$Q^2$")),
       color = "Model") +
  theme_minimal()

grid.arrange(R2_plot, Q2_plot, nrow = 1, widths = c(1.25, 1.8))
```


Here we can see the issues that arise with the method sPLS uses to choose the number of components to build. When ddsPLS and PLS start performing mean estimation, sPLS continues to build a model. This leads to a higher $R^2$ but a lower $Q^2$ meaning models perform worse on test data than mean estimation.

```{r, echo=FALSE, cache=TRUE}
ggplot(samp_test, aes(x = noise_weight, y = ncomp, color = model)) +
  geom_hline(yintercept = 2,
             size = 2,
             color = "gold3",
             alpha = 0.4) +
  geom_point(size = 5,
             alpha = 0.6) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = "Number of Components",
       color = "Model") +
  theme_minimal()
  
```


Here is the number of components built as the noise increases across model types. It is important to note that the sPLS tuning algorithm used always selects at least one component. When 0 components are included in the model this means that mean estimation performs better than the model built. Not building any components at high noise levels is not necessarily a bad sign as PLS methods are not designed to deal with problems due to high noise to signal ratios, it simply means that the noise included has drowned out the signal.

Here we can see that PLS tends to build more components than other models as we would expect since it makes no attempt to build sparser models.

Lets now compare model performance at fixed values of $\omega$ when $\textbf{A}$ is generated using the complex method and $\textbf{D}$ is generated using the 2-block method. For these trials we will fix $n = 100$, $p = 100$, and $q = 5$.

```{r, echo=FALSE}
noise_1_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_1_ddspls.csv")
noise_1_spls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_1_spls.csv")
noise_1_pls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_1_pls.csv")
noise_0.5_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_0.5_ddspls.csv")
noise_0.5_spls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_0.5_spls.csv")
noise_0.5_pls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/noise_0.5_pls.csv")

noise_1_ddspls <- noise_1_ddspls %>%
  mutate(model = "ddsPLS")
noise_1_spls <- noise_1_spls %>%
  mutate(model = "sPLS")
noise_1_pls <- noise_1_pls %>%
  mutate(model = "PLS")
noise_0.5_ddspls <- noise_0.5_ddspls %>%
  mutate(model = "ddsPLS")
noise_0.5_spls <- noise_0.5_spls %>%
  mutate(model = "sPLS")
noise_0.5_pls <- noise_0.5_pls %>%
  mutate(model = "PLS")

noise_1 <- rbind(noise_1_ddspls, noise_1_spls, noise_1_pls)
noise_0.5 <- rbind(noise_0.5_ddspls, noise_0.5_spls, noise_0.5_pls)

noise_1$model <- factor(noise_1$model,
    levels = c("ddsPLS", "sPLS", "PLS"),
    ordered = TRUE)

noise_0.5$model <- factor(noise_0.5$model,
    levels = c("ddsPLS", "sPLS", "PLS"),
    ordered = TRUE)
```

First let $\omega = 0.5$. At this noise level we can expect fairly robust and preictive models to be built.


```{r, echo=FALSE, cache=TRUE}
noise_0.5 %>%
  group_by(model) %>%
  ggplot(aes(x = ncomp, fill = model)) +
  geom_histogram(binwidth = 1) +
  scale_fill_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = "Number of Components",
       y = "Count",
       fill = "Model")+
  theme_minimal() +
  facet_wrap(vars(model))
```

Due to the shape of the data, models should include 2 components. ddsPLS and sPLS both tend to build the correct number of components with sPLS building 2 components every time. PLS always builds too many components and never builds the correct number.


```{r, echo=FALSE, cache=TRUE}
noise_0.5 %>%
  group_by(model) %>%
  ggplot(aes(x = model, y = RMSE, color = model)) +
  geom_boxplot() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = "Model",
       color = "Model") +
  theme_minimal()
```


At $\omega = 0.5$, we can see that ddsPLS and sPLS perform similarly in regards to to RMSE while PLS performs significantly better.



Now let $\omega = 1$. 


```{r, echo=FALSE, cache=TRUE}
noise_1 %>%
  group_by(model) %>%
  ggplot(aes(x = ncomp, fill = model)) +
  geom_histogram(binwidth = 1) +
  scale_fill_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = "Number of Components",
       y = "Count",
       fill = "Model")+
  theme_minimal() +
  facet_wrap(vars(model))
```



ddsPLS and sPLS still tend to build the correct number of components when $\omega$ is increased. Both show an increased tendency to build too few components. PLS now builds fewer components but still tends to build too many.


```{r, echo=FALSE, cache=TRUE}
noise_1 %>%
  group_by(model) %>%
  ggplot(aes(x = model, y = RMSE, color = model)) +
  geom_boxplot() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(x = "Model",
       color = "Model") +
  theme_minimal()
```

Again PLS tends to have the lowest RMSE however it's performance is now more similar to the other models than with less noise. ddsPLS and sPLS still perform very similarly however we can see that ddsPLS now performs slightly better.

```{r, echo=FALSE, cache=TRUE}
noise_1 %>%
  group_by(model) %>%
  ggplot(aes(x = R2, y = Q2, color = model)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point(alpha = 0.75) +
  geom_abline(slope = 1, intercept = 0) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(color = "Model",
       x = unname(TeX("$R^2$")),
       y = unname(TeX("$Q^2$"))) +
  theme_minimal()
```

```{r, echo=FALSE, cache=TRUE}
noise_1 %>%
  ggplot(aes(x = ncomp, y = R2, color = model)) +
  geom_vline(xintercept = 2,
             color = "gold3",
             alpha = 0.4,
             size = 2) +
  geom_point(size = 3, alpha = 0.3) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  labs(color = "Model",
       y = unname(TeX("$R^2$")),
       x = "Number of Components") +
  theme_minimal()
```

As this plot illustrates, there is a clear relationship between the number of components and the $R^2$ value. Almost all of the models that poorly explain the original data build only 1 component.

Here we will also introduce a problem that sparse models will suffer from. Looking at the results from PLS models, we see what we would hope to observe, models consistently have fairly high $R^2$ values while also having lower but still well performing $Q^2$ values. For the most part, ddsPLS and sPLS exhibit the same behavior however, we can notice an issue with the cluster of points in the lower left corner. These points exhibit cases where the original model failed to provide a good fit of the training data as evidenced by the noticeably lower $R^2$ value. While we expect sparse models to have a lower $R^2$ than PLS, the problems is that these models do not offer consistent performance on similar data.

Due to the nature of these models, I would guess that this is due to the models overly penalizing model complexity. Due to the frequency of these models that underfit the data ($12%$ of ddsPLS models and $16%$ of sPLS models) it is extremely unlikely that these results are due to random variation in the data that the different models fit. Since this problem is prevalent in both ddsPLS and sPLS, at least part of it is likely due to the models either heavily penalizing or selecting to remove relevant predictors.



## Predictors

```{r, echo = FALSE}
p_test_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/p_test_ddspls.csv")
p_test_spls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/p_test_spls.csv")
p_test_pls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/p_test_pls.csv")
p_test_ddspls_Q2 <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/p_test_ddspls_Q2.csv")

p_test_ddspls <- p_test_ddspls %>%
  mutate(model = "ddsPLS")
p_test_spls <- p_test_spls %>%
  mutate(model = "sPLS")
p_test_pls <- p_test_pls %>%
  mutate(model = "PLS")
p_test_ddspls_Q2 <- p_test_ddspls_Q2 %>%
  mutate(model = "ddsPLS(Q2)")

p_test <- rbind(p_test_ddspls, p_test_spls, p_test_pls, p_test_ddspls_Q2)

p_test$model <- factor(p_test$model,
    levels = c("ddsPLS", "sPLS", "PLS", "ddsPLS(Q2)"),
    ordered = TRUE)
```

The following are for data generated with $n = 50$, $q = 5$, $\omega = 1$, $\textbf{A}$ generated with a complex structure, and $\textbf{D}$ generated with the 2-block method.

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
p_test %>%
  filter(model != "ddsPLS(Q2)") %>%
  ggplot(mapping = aes(x = p, y = ncomp, color = model)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  scale_y_continuous(breaks = c(0:10)) +
  labs(y = "Number of Components",
       x = "Number of Predictors",
       color = "Model") +
  theme_minimal()
```

Here we can see the differing tendencies in the number of components that the models build. Ideally, models will build two components* in this situation. ddsPLS performs by far the best in this regard as PLS tends to overfit the data while sPLS underfits it.


```{r, echo=FALSE, cache=TRUE}
p_test %>%
  filter(model != "ddsPLS(Q2)") %>%
  ggplot(mapping = aes(x = p, y = rmse, color = model)) +
  geom_point() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  geom_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              size = 0.75) +
  labs(x = unname(TeX("$p$")),
       y = "RMSE") +
  theme_minimal()
```

As $p$ increases all PLS models show some degree of decline in performance as we would expect. Both of the sparse models exhibit a noticeable but fairly moderate increase in the RMSE. Meanwhile PLS exhibits a more drastic increase in its RMSE. 

```{r, echo=FALSE, cache=TRUE}
p_test %>%
  filter(model != "ddsPLS(Q2)") %>%
  ggplot(mapping = aes(x = p, y = R2.Q2, color = model)) +
  geom_point() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  geom_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              size = 0.75) +
  labs(x = unname(TeX("$p$")),
       y = unname(TeX("$R^2-Q^2$"))) +
  theme_minimal()
```

The increase in RMSE for the PLS model is due to it overfitting the data. As $p$ increases, so does the difference in $R^2$ and $Q^2$. This is due to PLS fitting random correlation that appears in the noise that the sparse models are able to mostly ignore. Since ddsPLS is the only model to use $R^2 - Q^2$ as the metric for parameter selection it is unsurprising that it is the only one of the three models for which the metric decreases.


```{r, echo=FALSE, cache=TRUE}
ddspls_1000_reps <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/ddspls_p_1000.csv")
spls_1000_reps <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/spls_p_1000.csv")
pls_1000_reps <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/pls_p_1000.csv")
ddspls_1000_reps_Q2 <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/ddspls_p_1000_Q2.csv")

ddspls_1000_reps <- ddspls_1000_reps %>%
  mutate(model = "ddsPLS")
spls_1000_reps <- spls_1000_reps %>%
  mutate(model = "sPLS")
pls_1000_reps <- pls_1000_reps %>%
  mutate(model = "PLS")
ddspls_1000_reps_Q2 <- ddspls_1000_reps_Q2 %>%
  mutate(model = "ddsPLS(Q2)")

reps_p <- rbind(ddspls_1000_reps, spls_1000_reps, pls_1000_reps, ddspls_1000_reps_Q2)

reps_p$model <- factor(reps_p$model,
    levels = c("ddsPLS", "sPLS", "PLS", "ddsPLS(Q2)"),
    ordered = TRUE)
```

The following table shows the results of 100 replications with $p = 1000$ for each model type for the RMSE.

```{r, echo=FALSE, cache=TRUE}
reps_p %>%
  filter(model != "ddsPLS(Q2)") %>%
  group_by(model) %>%
  summarise(Mean = round(mean(rmse), 3),
            "25th quantile" = round(quantile(rmse, 0.25), 3),
            "50th quantile" = round(median(rmse), 3),
            "75th quantile" = round(quantile(rmse, 0.75), 3),
            Var = round(var(rmse), 4)) %>%
  gt() %>%
  cols_label(model = "Model")
```

From this chart we can see that with a large number of predictors uncorrelated to the responses, ddsPLS performs significantly better than the other methods in regards to the RMSE. While the mean RMSE of PLS is significantly lower than that of sPLS the two often offer models that perform similarly. Furthermore, note the higher variance of the RMSE for ddsPLS models. This is representative of the performance of ddsPLS in general as models tend to be more variable in their performance on the test data.

```{r, echo=FALSE, cache=TRUE}
reps_p %>%
  filter(model != "ddsPLS(Q2)") %>%
  ggplot(mapping = aes(x = ncomp, fill = model)) +
  geom_histogram(binwidth = 1) +
  scale_fill_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  scale_x_continuous(n.breaks = 8) +
  labs(x = "Number of Components",
       y = "Count",
       fill = "Model") +
  theme_minimal() +
  facet_wrap(vars(model))
```

When looking at the number of components built during the replications, we can see that ddsPLS is the only model that tends to build the correct number of components. As in previous trials, sPLS uses too few components while PLS builds too many.

## Responses

```{r, echo=FALSE}
q_test_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/q_test_ddspls.csv")
q_test_spls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/q_test_spls.csv")
q_test_pls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/q_test_pls.csv")

q_test_ddspls <- q_test_ddspls %>%
  mutate(Model = "ddsPLS")
q_test_spls <- q_test_spls %>%
  mutate(Model = "sPLS")
q_test_pls <- q_test_pls %>%
  mutate(Model = "PLS")

q_test <- rbind(q_test_ddspls, q_test_spls, q_test_pls)

q_test$Model <- factor(q_test$Model,
    levels = c("ddsPLS", "sPLS", "PLS"),
    ordered = TRUE)
```

The following are for data generated with $n = 50$, $p = 100$, $\omega = 0.5$, $\textbf{A}$ generated with a simple structure, and $\textbf{D}$ generated with the simple method.

```{r, echo=FALSE, cache=TRUE}
ggplot(q_test, aes(x = q, y = RMSE, color = Model)) +
  geom_point(alpha = 0.8) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) + 
  theme_minimal()
```

As expected, the RMSE increases as the number of response variables increase. More importantly, we can see that all models continue to perform similarly as the number of response variables increases.

```{r, echo=FALSE, cache=TRUE}
ggplot(q_test, aes(x = q, y = R2, color = Model)) +
  geom_point(alpha = 0.8) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) + 
  theme_minimal()
```

As $q$ increases, we see all models perform similarily in their ability to fit the data.


```{r, echo=FALSE}
ggplot(q_test, aes(x = q, y = ncomp, color = Model)) +
  geom_hline(yintercept = 1,
             size = 2,
             color = "gold3",
             alpha = 0.5) +
  geom_point(size = 3, alpha = 0.4) +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) + 
  labs(y = "Number of Components") +
  theme_minimal()
```

Models tend to build the correct number of components in this simple case


```{r, echo=FALSE}
ddspls_q_25 <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/ddspls_q_25.csv")
spls_q_25 <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/spls_q_25.csv")
pls_q_25 <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/pls_q_25.csv")

ddspls_q_25 <- ddspls_q_25 %>%
  mutate(Model = "ddsPLS")
spls_q_25 <- spls_q_25 %>%
  mutate(Model = "sPLS")
pls_q_25 <- pls_q_25 %>%
  mutate(Model = "PLS")

q_25 <- rbind(ddspls_q_25, spls_q_25, pls_q_25)

q_25$Model <- factor(q_25$Model,
    levels = c("ddsPLS", "sPLS", "PLS"),
    ordered = TRUE)
```

The following results are for $q=25$. Not sure how data was generated.

```{r, echo=FALSE}
ggplot(q_25, aes(x = ncomp, fill = Model)) +
  geom_histogram(binwidth = 1) +
  scale_fill_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  theme_minimal() +
  facet_wrap(vars(Model))
```

```{r, echo=FALSE}
ggplot(q_25, aes(x = RMSE, fill = Model)) +
  geom_histogram(binwidth = 0.05, position = "identity", alpha = 0.8) +
  scale_fill_manual(values = c(ddsPLS = "turquoise3",
                                sPLS = "tomato2",
                                PLS = "darkolivegreen3")) +
  theme_minimal()
```

```{r, echo=FALSE}
q_25 %>%
  group_by(Model) %>%
  summarise(Mean = round(mean(RMSE), 3),
            "25th quantile" = round(quantile(RMSE, 0.25), 3),
            "50th quantile" = round(median(RMSE), 3),
            "75th quantile" = round(quantile(RMSE, 0.75), 3),
            Var = round(var(RMSE), 4)) %>%
  gt()
```

## ddsPLS Parameter Selection


```{r, echo=FALSE}
p_test %>%
  filter(model %in% c("ddsPLS", "ddsPLS(Q2)")) %>%
  ggplot(mapping = aes(x = p, y = R2.Q2, color = model)) +
  geom_point() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                "ddsPLS(Q2)" = "tan4")) +
  geom_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              size = 0.75) +
  labs(x = unname(TeX("$p$")),
       y = unname(TeX("$R^2-Q^2$"))) +
  theme_minimal()

p_test %>%
  filter(model %in% c("ddsPLS", "ddsPLS(Q2)")) %>%
  ggplot(mapping = aes(x = p, y = rmse, color = model)) +
  geom_point() +
  scale_color_manual(values = c(ddsPLS = "turquoise3",
                                "ddsPLS(Q2)" = "tan4")) +
  geom_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              size = 0.75) +
  labs(x = unname(TeX("$p$")),
       y = "RMSE") +
  theme_minimal()
```