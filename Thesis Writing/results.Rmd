---
title: "Results"
author: "Harpeth Lee"
date: "4/2/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE}
library(ggplot2)
library(latex2exp)
library(ggpmisc)
library(gridExtra)
```


# Results

## Data Simulation

Our data simulations are done based on the latent variable model that PLS methods assume. We generate $\textbf{X}$ and $\textbf{Y}$ using the following equations, $\textbf{X} = \boldsymbol{\phi} \textbf{A} + \boldsymbol{\epsilon}_X$ and $\textbf{Y} = \boldsymbol{\phi} \textbf{D} + \boldsymbol{\epsilon}_Y$. The $\textbf{A}$ and $\textbf{D}$ matrices are fixed, providing most of the structure seen in $\textbf{X}$ and $\textbf{Y}$. $\boldsymbol{\phi}$, $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$ are randomly generated. The method by which $\boldsymbol{\phi}$ is generated is less important as we want to approximate $\boldsymbol{\phi}$ with our final model. $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$ are sampled from a normal distribution.

Although $\boldsymbol{\phi}$ is generated using a similar method as $\boldsymbol{\epsilon}_X$, and $\boldsymbol{\epsilon}_Y$, it is important to note that $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ are noise while $\boldsymbol{\phi}$ is not. Ideally, our models will be able to identify $\boldsymbol{\phi}$ while ignoring $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$. $\boldsymbol{\phi}$ is generated sampling from a multivariate normal distribution with uncorrelated samples(using the `mvrnorm` function in the `MASS` package in order to make sure columns are uncorrelated). $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ are generated by samples from random distributions (using `rnorm`) as this will cause some random correlation in the noise. This random correlation is included as we want to test model's ability to distinguish true correlation from random correlation.

When generating the data, there are four variables related to the dimensionality of the matrices which must be chosen, $n$, $p$, $q$, and $R$. $n$ is the number of observations generated, $p$ is the number of predictors, and $q$ is the number of response variables. Of these variables, $R$ is the only one of these variables that is not clear from the generated data. $R$ is the size of the dimension of $\textbf{A}$,$\textbf{D}$, and $\boldsymbol{\phi}$ that is lost when matrix multiplication is performed. Since we want $\textbf{X}$ to be of dimension $n \times p$ and $\textbf{Y}$ to be of dimension $n \times q$, $\boldsymbol{\phi}$ will be of dimension $n \times R$, $\textbf{A}$ will be of dimension $R \times p$, and $\textbf{D}$ will be of dimension $R \times q$. Although it may seem like we will want our final model to have $R$ total components, we will want $K$ to be the number of eigenvectors of the variance matrix of $\textbf{X}$ that have non-null projections onto the covariance matrix of $\textbf{X}$ and $\textbf{Y}$. [Helland and Almoy]

There were a number of methods used to generate these matrices. $\boldsymbol{\phi}$ was always generated from uncorrelated normal distributions. Two methods were used to generate $\textbf{A}$, the "simple" method and the "complex" method. In the simple method, 
$$
\textbf{A} =
\begin{pmatrix}
\textbf{1}_{\frac{3}{5}R \times 5} & \textbf{0}_{\frac{3}{5}R \times 10} & \textbf{0}_{\frac{3}{5}R \times (p-15)}  \\
\textbf{0}_{\frac{2}{5}R \times 5} & \textbf{1}_{\frac{2}{5}R \times 10} & \textbf{0}_{\frac{2}{5}R \times (p-15)}
\end{pmatrix}
$$
Note that $p \geq 15$ in order for this method to work. The dimensions of $\frac{3}{5}R$ and $\frac{2}{5}R$ are not exact, we round and then take the difference in order to make sure values are integers that add up to R.

Using the "complex" method,
$$
\textbf{A} = 
\begin{pmatrix}
\textbf{1}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 2} & \textbf{0}_{3 \times (p-23)} \\
\textbf{0}_{2 \times 5} & \textbf{1}_{2 \times 10} & \textbf{0}_{2 \times 5} & \textbf{0}_{2 \times 1} & \textbf{0}_{2 \times 2} & \textbf{0}_{2 \times (p-23)} \\
\textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{1}_{3 \times 5} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 2} & \textbf{0}_{3 \times (p-23)} \\
\textbf{0}_{2 \times 5} & \textbf{0}_{2 \times 10} & \textbf{0}_{2 \times 5} & \textbf{1}_{2 \times 1} & \textbf{0}_{2 \times 2} & \textbf{0}_{2 \times (p-23)}\\
\textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 10} & \textbf{0}_{3 \times 5} & \textbf{0}_{3 \times 1}  & \textbf{1}_{3 \times 2} & \textbf{0}_{3 \times (p-23)}
\end{pmatrix}
$$
Note that $p \geq 23$ in order for this method to work. Furthermore, $R=13$ whenever this method is used.

A wider number of methods were used to generate the $\textbf{D}$ matrix. We will call these methods "basic", "diagonal", "simple", "2-block", and "4-block". In the basic method, 
$$
\textbf{D} = \textbf{1}_{R \times q}
$$
Using the "diagonal" method,
$$
\textbf{D} = \textbf{I}_{R \times q}
$$
Using the "simple" method
$$
\textbf{D} = 
\begin{pmatrix}
\textbf{1}_{R \times 1} & \textbf{0}_{R \times (q-1)}
\end{pmatrix}
$$
Using the "2-block" method
$$
\textbf{D} = 
\begin{pmatrix}
\textbf{1}_{\frac{3}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{3}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{3}{5} \times \frac{1}{2}q} \\
\textbf{0}_{\frac{2}{5}R \times \frac{1}{4}q} & \textbf{1}_{\frac{2}{5}R \times \frac{1}{4}q} & \textbf{0}_{\frac{2}{5} \times \frac{1}{2}q}
\end{pmatrix}
$$
Note the two blocks of 1s hence the name "2-block" method.

Using the "4-block" method
$$
\textbf{D} =
\begin{pmatrix}
\textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
 \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{0}_{3 \times 1} & \textbf{1}_{3 \times 1} & \textbf{0}_{3 \times (q-4)} \\
\textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times 1} & \textbf{0}_{1 \times (q-4)} & 
\end{pmatrix}
$$
Note that $R=13$ at all times when the "4-block" method is used.

The $\boldsymbol{\epsilon}_X$ and $\boldsymbol{\epsilon}_Y$ matrices are generated from samples from a standard normal distribution and then scaled by a factor $\omega$, called the noise weight. $\boldsymbol{\epsilon}_X$ is of dimension $n \times p$ and $\boldsymbol{\epsilon}_Y$ is of dimension $n \times q$. Since the noise is randomly generated we expect to see some correlation occur both between predictors and between the predictors and the responses.


## Noise

When examining model performance, it is important to consider how it performs under conditions where the amount of noise varies. The following graphs show the relationship between $\omega$ and the RMSE for the ddsPLS, sPLS, and PLS methods. Note the extremely linear relation displayed in all three.

```{r, fig.align='center', echo=FALSE, out.width="40%"}
samp_test_ddspls <- read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/ddspls_noise_weight_1.csv")

ggplot(samp_test_ddspls, aes(x = noise_weight, y = RMSE)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", size = 0.5, se= FALSE, formula = y ~ x) +
   stat_poly_eq(formula = y~x, 
                aes(label = paste(..eq.label..)), 
                parse = TRUE) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       title = "ddsPLS", 
       caption = unname(TeX("Relation between $\\omega$ and the test set RMSE for the ddsPLS model. $$n=100, p=100, q=5$$"))) +
  theme_minimal()
```

```{r, echo=FALSE, out.width="40%"}
samp_test_spls <-read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/spls_noise_weight_1.csv")[,-1]
colnames(samp_test_spls) <- c("noise_weight", "ncomp", "RMSE", "R2", "Q2", "R2-Q2")

ggplot(samp_test_spls, aes(x = noise_weight, y = RMSE)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", size = 0.5, se= FALSE, formula = y ~ x) +
   stat_poly_eq(formula = y~x, 
                aes(label = paste(..eq.label..)), 
                parse = TRUE) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       title = "sPLS", 
       caption = unname(TeX("Relation between $\\omega$ and the test set RMSE for the sPLS model. $$n=100, p=100, q=5$$"))) +
  theme_minimal()
```
```{r, echo=FALSE, out.width="40%"}
samp_test_pls <-read.csv2("/Users/johnlee/R Files/thesis-lee/Simulations/data/pls_noise_weight_1.csv")

ggplot(samp_test_pls, aes(x = noise_weight, y = RMSE)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", size = 0.5, se= FALSE, formula = y ~ x) +
   stat_poly_eq(formula = y~x, 
                aes(label = paste(..eq.label..)), 
                parse = TRUE) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       title = "PLS", 
       caption = unname(TeX("Relation between $\\omega$ and the test set RMSE for the PLS model. $$n=100, p=100, q=5$$"))) +
  theme_minimal()
```
Note the similar slopes of the line of best fit for all model types.


```{r, echo=FALSE, out.width="40%"}
ggplot(samp_test_ddspls, aes(x = noise_weight, y = ncomp)) +
  geom_point(size = 5) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = "Number of Components",
       title = "ddsPLS") +
  ylim(c(0,6)) +
  theme_minimal()
```
```{r, echo=FALSE, out.width="40%"}
ggplot(samp_test_spls, aes(x = noise_weight, y = ncomp)) +
  geom_point(size = 5) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = "Number of Components",
       title = "sPLS") +
  ylim(c(0,6)) +
  theme_minimal()
```
```{r, echo=FALSE, out.width="40%"}
ggplot(samp_test_pls, aes(x = noise_weight, y = ncomp)) +
  geom_point(size = 5) +
  labs(x = unname(TeX("Noise Weight ($\\omega$)")),
       y = "Number of Components",
       title = "PLS") +
  ylim(c(0,6)) +
  theme_minimal()
```
```{r}
grid.arrange(plot1, plot2, ncol=2)
```


Here is the number of components built as the noise increases across model types. It is important to note that the sPLS tuning algorithm used always selects at least one component. When 0 components are included in the model this means that mean estimation performs better than the model built. Not building any components at high noise levels is not necessarily a bad sign as PLS methods are not designed to deal with problems due to high noise to signal ratios, it simply means that the noise included has drowned out the signal.




## Predictors



