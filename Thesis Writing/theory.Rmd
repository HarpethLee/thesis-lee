---
title: "Theory"
author: "Harpeth Lee"
date: "3/15/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Theory


## Principal Component Analysis

Due to the similarity between partial least squares and principal component regression, we will briefly review principal component analysis. 

Principal component analysis(PCA) is a commonly used unsupervised learning technique. PCA works by finding the principle components of a dataset, these can be thought of as the direction along which the data varies the most in the feature space. For those of you with a background in linear algebra, the principal components will be the eigenvectors of the covariance matrix in order of the norm of the eigenvalues.


  PCA is one of the most commonly used dimension reduction techniques as it is often able to capture a large amount of the variation in a data set using only a few features. PCA returns a series of principal components, these can be thought of as vectors in the original feature space. Principal components will be orthogonal to each other so the variance explained by each component will be unique. Given an $n \times p$ data set, $n$ principal components can be created where each principal component is a vector of length $p$.
    
    
  The algorithm for finding the first principal component of a standardized $n \times p$ data matrix $\textbf{X}$ is as follows:
$$
\textbf{w}_1 = \text{argmax}_{||\textbf{w}||=1} \Big\{ \textbf{w}^{\text{T}} \textbf{X}^{\text{T}}\textbf{X} \textbf{w} \Big\}
$$


Note that $\textbf{X}^T\textbf{X}$ is the covariance matrix for $\textbf{X}$. For all following components, we will find the matrix $\hat{\textbf{X}}_k$ such that all variance explained by the first $k-1$ principal components is removed. This is done by finding
$$
\hat{\textbf{X}}_k = \textbf{X} - \sum_{j = 1}^{k-1} \textbf{X} \textbf{w}_j \textbf{w}_j^{\text{T}}
$$
We then use the same equation used to find $\textbf{w}_1$ to find the $k$th principle component, replacing $\textbf{X}$ with $\hat{\textbf{X}}_k$.

The following plot shows the first two(and only) principals components for a set of two dimensional data. The red line is the first principal component while the green line is the second. Note that they are perpendicular. The red line describes as much variation in the data using a singular vector in the original space. Although it may look similar, the first principal component is not the same as the regression line which minimizes the residual distance.

```{r, echo=FALSE}
library(MASS)

rand_data <- mvrnorm(n = 20, mu = c(4,1), Sigma = matrix(c(8, 5, 5, 4), nrow = 2))

pca_data <- prcomp(rand_data, scale. = TRUE)

plot(scale(rand_data), ann = FALSE)
abline(a = 0, b = pca_data$rotation[1,1]/pca_data$rotation[2,1], col = "red")
abline(a = 0, b = pca_data$rotation[1,2]/pca_data$rotation[2,2], col = "green")
```

Often principal components can be used as latent variables. Sometimes they may be meaningful and correspond to a property of the data that isn't directly measured. For example, the first principal component of data containing the nutritional values for food may correspond to how rich in vitamins a food is.

Principal Component Regression (PCR) is a regression technique that uses principal components as predictors. To perform PCR, one first finds the desired number of principal components and then performs linear regression using the selected principal components as predictors. Letting $\textbf{w}_1, \textbf{w}_2,...,\textbf{w}_k$ be the first $k$ principal components of our predictors $\textbf{X}$. This will take the form 
$$
\textbf{y} = \theta_0 + \sum_{i = 1}^k \theta_i \textbf{w}_i
$$

## Latent Variable Models

Latent variable models such as PCR and PLS are built on the assumption that there exists a structure linking the predictors and response variables that isn't directly captured in the data. There are two ways we can try and conceptualize these latent variables, either as being derived from the observed data or being used to generate the observed data. In this case, it is most helpful to think of latent variables as generating the observed data.

We will assume that $\textbf{X} = \phi \textbf{A} + \epsilon_X$ and $\textbf{Y} = \phi \textbf{D} + \epsilon_Y$. Here $\phi$ is a matrix of latent variables linking $\textbf{X}$ and $\textbf{Y}$. $\textbf{A}$ and $\textbf{D}$ are the transformation applied to $\phi$ to generate $\textbf{X}$ and $\textbf{Y}$. $\epsilon_X$ and $\epsilon_Y$ are random noise.

$\textbf{X} \in \mathbb{R}_{n \times p}$ and $\textbf{Y} \in \mathbb{R}_{n \times q}$. Thus, $\phi \in \mathbb{R}_{n \times \mathcal{K}}$, $\textbf{A} \in \mathbb{R}_{\mathcal{K} \times p}$,and $\textbf{D} \in \mathbb{R}_{\mathcal{K} \times q}$. Note that $\mathcal{K} \neq K$ where $K$ is the number of components that will be used in a PLS model.

Since all possible variance is explained by $\phi$, $\textbf{A}$, and $\textbf{D}$ we will find that $\text{Cov}(\phi, \epsilon_X) = \text{Cov}(\phi, \epsilon_Y) = \text{Cov}(\epsilon_X, \epsilon_Y) = 0$. Furthermore, $\text{Var}(\textbf{X}) = \textbf{A}\textbf{A}^\text{T} + \text{Var}(\epsilon_X)$ and $\text{Var}(\textbf{Y}) = \textbf{D}\textbf{D}^\text{T} + \text{Var}(\epsilon_Y)$. Finally, $\text{Cov}(\textbf{X}, \textbf{Y}) = \textbf{D}^{\text{T}}\textbf{A}$.

While it may seem intuitive to build a model of the form $\textbf{Y} = \textbf{B} \textbf{X} + \epsilon$ where $\textbf{B}$ satisfies $\textbf{A}\textbf{B} = \textbf{D}$. This only provides a unique answer under certain circumstances. Instead we want to formulate a model that is more similar to how the data is structured Thus, our model will be of the form $\textbf{X} = \textbf{t} \textbf{P} + \epsilon_X$ and $\textbf{Y} = \textbf{t} \textbf{C} + \epsilon_Y$.



## Partial Least Squares

Partial Least Squares(PLS) is a regression technique similar to PCR. Both techniques work by finding latent variables in the original data with which to perform linear regression. PLS is considered to be one of the best tools for modeling the underlying structure between $\textbf{X}$ and $\textbf{Y}$ in feature space. As suggested its creator, the name "Projection onto Latent Structures" is a more descriptive acronym as the original data is projected onto latent structures which are then used for linear regression.

Unlike PCR, PLS creates latent variables using the covariance matrix instead of the variance matrix. Implicit in PCR is the assumption that direction of high variance among the predictors explains much of the variance among the response variables. While this assumption is usually reasonable, PLS circumvents this assumption by directly building the latent variables along the direction in feature space along where the responses vary the most along the predictors.

PLS is designed to model situations where there are a high number of predictors in relation to the number of observations($n \approx p/n < p$). In these situation, most models will struggle to differentiate noise from signal and will be highly variable depending on the data used to train them. PLS circumvents this problem by simplifying the data into a smaller number of latent variables that still strongly reflect the original data set. Unlike models that depend solely on variable selection, PLS will not discard relevant information from all observed variables in order to fit a model.

PLS is also designed to perform well in situations where predictors are highly correlated and noisy. The data is projected onto underlying structures in order to handle correlation between predictors. Correlated predictors can often be better explained along a singular vector in feature space that captures all correlated predictors. This correlation will be captured in the latent structures that the data is project onto. These underlying structures captured in the latent variables will also be less sensitive to noise among the predictors.

As previously mentioned, PLS models often follow a five step process outline by Lorenzo. To briefly review, these steps are a) estimate the covariance matrix, b) find the space associated with the first eigenvector, c) project the covariate and response matrices into the space from the previous step, d) perform linear regression between the projection of the covariate and response matrices, and e) remove the predicted information.


#### NIPALS-PLS


The NIPALS-PLS algorithm (sometimes referred to as the PLS2 algorithm) is the first PLS model suggested by Wold and still one of the more commonly used PLS models. Similar basic PLS models almost exclusively differ in the method used to calculate the covariance matrix, especially when dealing with missing data.

Before going into detail on the model it is important to note that all PLS algorithms require the data to be centered and standardized in order to prevent variables with different scales from exhibiting an outsize impact on the final outcome.

We will assume that $\textbf{M} = \frac{1}{n} \sum_{i = 1}^n (\textbf{y}_i - \bar{\textbf{y}}) (\textbf{x}_i - \bar{\textbf{x}})^{\text{T}}$ will be used to estimate the covariance matrix unless otherwise noted. This estimate, $\textbf{M}$ is the empirical covariance matrix calculated using the observed data. Estimating $\textbf{M}$ is the most sensitive step as this estimation is sensitive to the curse of dimensionality.

The NIPALS-PLS algorithm is as follows:

For all $k \in [1:K]$,

a) $\textbf{u}_k = \overrightarrow{\text{RSV}}(\textbf{M}_k)$

b) $\textbf{t}_k = \textbf{X}_k \textbf{u}_k$

c) $\textbf{p}_k = \frac{\textbf{X}_k^{\text{T}}\textbf{t}_k}{\textbf{t}_k^{\text{T}}\textbf{t}_k}$

d) $\textbf{c}_k = \frac{\textbf{Y}_k^{\text{T}}\textbf{t}_k}{\textbf{t}_k^{\text{T}}\textbf{t}_k}$

e) $\textbf{X}_{k + 1} = \textbf{X}_k - \textbf{t}_k \textbf{p}_k^{\text{T}}$, $\textbf{Y}_{k + 1} = \textbf{Y}_k - \textbf{t}_k \textbf{c}_k^{\text{T}}$

Walking through these steps; a) finds the vector that explains the most variance in the covariance matrix for the remaining data, b) estimates the score, c) and d) estimate the linear regression matrices for $\textbf{X}$ and $\textbf{Y}$, and e) deflates the data.


The NIPALS-PLS algorithm maximizes $\textbf{v}^\text{T}\textbf{M}_k \textbf{u}$ with the constraint $\textbf{v}^\text{T} \textbf{v} = \textbf{u} \textbf{u}^\text{T} = 1$. The only parameter that needs tuning is $K$, the number of components used in the model.


#### sPLS

Here we will discuss a sparse PlS algorithm outlined by Le Cao et.al. It is important to note that there exist other sparse PLS algorithms most notably the one created by Chun and Keles.

First we must define the soft-thresholding function $S_{\lambda}(\textbf{M}) = \text{argmin} \left( ||\textbf{M} - \boldsymbol{\Sigma}||^2 + 2\lambda|\boldsymbol{\Sigma}| \right)$ with $\boldsymbol{\Sigma} \in \mathbb{R}^{q \times p}$ and where $\textbf{M}$ is again our estimate of the covariance matrix.

The sPLS algorithm is as follows:

For all $k \in [1:K]$,

$\text{a}^{\dagger}$) Start with initial $\textbf{v}_k$ and iterate until the two steps converge.

  $\text{i}^{\dagger}$) $\textbf{u}_k = S_{\lambda_u^k}(\textbf{M}_k^\text{T} \textbf{v}_k)$, $\textbf{u}_k = \frac{\textbf{u}_k}{||\textbf{u}_k||}$
  
  $\text{ii}^{\dagger}$) $\textbf{v}_k = S_{\lambda_v^k}(\textbf{M}_k^\text{T} \textbf{u}_k)$, $\textbf{v}_k = \frac{\textbf{v}_k}{||\textbf{v}_k||}$


b) $\textbf{t}_k = \textbf{X}_k \textbf{u}_k$

c) $\textbf{p}_k = \frac{\textbf{X}_k^{\text{T}}\textbf{t}_k}{\textbf{t}_k^{\text{T}}\textbf{t}_k}$

d) $\textbf{c}_k = \frac{\textbf{Y}_k^{\text{T}}\textbf{t}_k}{\textbf{t}_k^{\text{T}}\textbf{t}_k}$

e) $\textbf{X}_{k + 1} = \textbf{X}_k - \textbf{t}_k \textbf{p}_k^{\text{T}}$, $\textbf{Y}_{k + 1} = \textbf{Y}_k - \textbf{t}_k \textbf{c}_k^{\text{T}}$

Notice that step a) is the only step that differs from the NIPALS-PLS algorithm. This is where sparsity is imposed upon $\textbf{u}$ and $\textbf{v}$ which are used to calculate the score and loadings. Note that the sparsity is only directly imposed on $\textbf{X}$.

The algorithm minimizes $||(n-1)\textbf{M}_k - \textbf{v}\textbf{u}^\text{T}||^2 + \lambda_u^k|\textbf{u}| + \lambda_v^k|\textbf{v}|$ for $\textbf{u}$ and $\textbf{v}$ with the constraint $\textbf{v}^\text{T} \textbf{v} = \textbf{u} \textbf{u}^\text{T} = 1$. sPLS has $2K + 1$ parameters with two tuning parameters for each component and then the number of components.

#### Data-driven Sparse Partial Least Squares

Data-driven sparse partial least squares(ddsPLS) is a new PLS method recently proposed by Lorenzo. Unlike previous sparse partial least squares algorithms, ddsPLS directly imposes sparisty on the empirical covariance matrix instead of its eigenvector decomposition.

a) $\normalsize \textbf{u}_k = \overrightarrow{RSV}(S_{\lambda_k}(M_{k}))$, $\normalsize \textbf{v}_r = \overrightarrow{RSV}(S_{\lambda^{(r)}}({M^{(r)}}^T))$

b) $\normalsize \textbf{t}_k = \textbf{X}_{k} \textbf{u}_k$

c) $\normalsize \textbf{p}_k = \frac{{\textbf{X}_{k}}^T \textbf{t}_k}{{\textbf{t}_k}^T \textbf{t}_k}$

d) $\normalsize \Pi_k = \text{diag}\left(\{\delta_{\neq 0}(\textbf{v}_k)_j \}_{j \in [1:q]} \right)$
  $\normalsize \textbf{c}_k = \text{argmin}|| \textbf{Y}_{k} \Pi_r - \textbf{t}_k \textbf{V}^T||^2 = \frac{({\textbf{Y}_{k} \Pi_k})^T \textbf{t}_k}{{\textbf{t}_k}^T \textbf{t}_k}$

e) $\normalsize \textbf{X}_{k+1} = \textbf{X}_{k} - \textbf{t}_k \textbf{p}_k^T$
  $\normalsize \textbf{Y}_{k+1} = \textbf{Y}_{k} - \textbf{t}_k \textbf{c}_k^T$
  
Note that if $\lambda = 0$ the outcome is identical to the NIPALS-PLS algorithm.

ddsPLS maximizes $\textbf{v}^\text{T} S_{\lambda_k}(\textbf{M}_k^\text{T}) \textbf{u}$ for $\textbf{u}$ and $\textbf{v}$ with the constraint $\textbf{v}^\text{T} \textbf{v} = \textbf{u} \textbf{u}^\text{T} = 1$. The ddsPLS algorithm has $K + 1$ parameters as there is only one parameter that needs to be maximized for each component. This gives it a better computation time when compared to other sparse PLS algorithms as cross validation or bootstrapping only needs to be performed for one parameter per component.

