---
title: "Journal"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To do List
1. Find data
2. Read chapter 2 of Advances in PCA
3. Learn more about R packages.
4. Figure out which variables from songs to keep
5. Connect spyder to pip
6. Audio interface in R

Look into combinations of SPCA and KPCA. Would the transformations applied during KPCA make PCs too hard to interpret or could we find meaningful interpretations of a small number of variables under a transformation.

KPCA works best with large amount of data fairly evenly distributed along the manifold.

I was able to find a few (I think 3) papers on Sparse Kernel PCA (SKPCA). Performs KPCA but adds a penalty similar to LASSO in order to give sparser PCs.

Look into other ways to penalize model complexity and test them with SPCA/SKPCA.

I want to look further into different methods of penalizing model complexity. This will likely mean focusing on SPCA right now and later adapting it to KPCA.

Would it be possible to find a way to create principal components that tend to be sparse and are still orthogonal?

How does SPCArt help with interpretability, it seems like results would be harder to interpret due to the rotation that the data undergoes, undercutting one the main advantages of SPCA.



Topic Idea:

Look into how different methods of sparse principal component analysis effect the performance of principal component regression (sparse principal component regression) to see if or how they can improve model performance. Based on preliminary research, it looks like there is a fair amount of recent research in SPCR so combining with more recently proposed SPCA models should yield novel results. SPCR should offer improvements in model complexity while possibly being less likely to overfit training data.

Maybe start by testing SPCR using SPCArt as a method for generating sparse principal components.

This blog feed back [https://archive.ics.uci.edu/ml/datasets/BlogFeedback] data set from the UCI repository might work, lots of attributes.


```{r}
library(spcr)

spcr <- spcr(as.matrix(train[1:50,1:280]), train[1:50,281], 5,
                lambda.B = 2.5, lambda.gamma = 2.5)
```

`spcr` function from the `spcr` package works to perform SPCR in R.
[https://cran.r-project.org/web/packages/spcr/spcr.pdf]


```{r}
test_preds <- as.matrix(test[,1:280])
test_resp <- as.matrix(test[, 281])
pcs <- test_preds%*%spcr$loadings.B
```

```{r}
resp <- apply(pcs, 1, function(x) {
  crossprod(x, spcr$gamma)
})
sum(resid^2)/length(resid)
```

```{r}
spcr2 <- spcr(as.matrix(train[1:50,1:280]), train[1:50,281], 10,
                lambda.B = 2.5, lambda.gamma = 2.5)
```

```{r}
pcs2 <- test_preds%*%spcr2$loadings.B

resp2 <- apply(pcs2, 1, function(x) {
  crossprod(x, spcr2$gamma)
})

resid2 <- test_resp - resp2
sum(resid2^2)/length(resid2)
```

```{r}
spcr_resid <- function(k = 5, lambda.B = 0.5, lambda.gamma = 0.5) {
 
  spcr_model <- spcr(as.matrix(train[1:50,1:280]), train[1:50,281], k,
                lambda.B, lambda.gamma = 2.5)

  prin_comps <- test_preds%*%spcr_model$loadings.B%*%spcr_model$gamma + spcr_model$gamma0

  resp <- prin_comps

  resid <- test_resp - resp
  sum(resid^2)/length(resid) 
}
```

```{r}
spcr_resid2 <- function(k = 5, lambda.B = 0.5, lambda.gamma = 0.5, w = 0.1, xi = 0.01) {
 
  spcr_model <- spcr(as.matrix(train[1:50,1:280]), train[1:50,281], k,
                lambda.B, lambda.gamma = 2.5)

  ##Problem here with reconstructing principle components will look more closely at the algorithm
  prin_comps <- as.matrix(train[1:50,1:280])%*%spcr_model$loadings.B%*%spcr_model$gamma

  resp <- prin_comps + spcr_model$gamma0

  resid <- train[1:50,281] - resp
  sum(resid^2)/length(resid) 
}
```

```{r}
plot(1:10, lapply(1:10, function(a){spcr_resid2(a)}))
```
I'm not sure what is going on as it seems that including more principle components decreases model performance on the training data I should look into whether or not this is a problem with the function's implementation or a feature that SPCR can sometimes exhibit.


Look into partial least squares, try cross validation with data.


```{r}
library(pls)
```

```{r}
model1 <- plsr(as.matrix(train[1:100,281])~as.matrix(train[1:100,1:280]),
               ncomp = 25,
               data = train,
               validation = "CV")
```


```{r}
plot(RMSEP(model1), legendpos = "topright")
```
```{r}
model2 <- plsr(as.matrix(train[1:200,281])~as.matrix(train[1:200,1:280]),
               ncomp = 25,
               data = train,
               validation = "CV",
               sclae = TRUE)
```

```{r}
plot(RMSEP(model2), legendpos = "topright")
```


```{r}
pcr_model <- pcr(Y~.,
               ncomp = 25,
               data = train,
               validation = "CV")
```
```{r}
plot(RMSEP(pcr_model))
```
```{r}
pcr_preds <- predict(pcr_model, ncomp = 10, newdata = test_preds)
```

```{r}
pcr_resids <- pcr_preds[,1,1]- test$Y
```

```{r}
plot(x = test_resp, y = pcr_resids)
```




Problem with scaling the variables, looks like smaller subsets will have predictors which only take one value meaning they can't be scaled. Scale test set based on results from the training set.

Find a way to save mean and variance of the training data and then use those to scale the test data.


## Code for Scaling Test+Training Data

```{r}
train_means <- apply(train[,1:280], 2, function(x){
  c(mean(x),var(x))
})
```


```{r}
train_scale <- train

for(i in 1:ncol(train_means)) {
  if(train_means[2,i]==0)
     train_scale[,i] <- rep(NA, times = nrow(train))
  else
    train_scale[,i] <- (train[,i]-train_means[1,i])/train_means[2,i]
}
```

```{r}
library(dplyr)
train_scale <- train_scale %>%
    select_if(~ !any(is.na(.)))
```

```{r}
test_scale <- test

for(i in 1:ncol(train_means)) {
  if(train_means[2,i]==0)
     test_scale[,i] <- rep(NA, times = nrow(test))
  else
    test_scale[,i] <- (test[,i]-train_means[1,i])/train_means[2,i]
}
```

```{r}
test_scale <- test_scale %>%
    select_if(~ !any(is.na(.)))
```


For some reason the variance of `X13` is 0 in the training data but not in the scaled training data. I should probably restart the process of scaling and building the model. Will have to remove variables from the training data. Possible that variable names were shifted at some point.




```{r}
model3 <- plsr(Y~.,
               ncomp = 25,
               data = train_scale,
               validation = "CV")
```

```{r}
plot(RMSEP(model3), legendpos = "topright")
```

```{r}
mdl3_preds <- predict(model3, ncomp = 10, newdata = test_scale)
```

```{r}
mdl3_resids <- mdl3_preds[,1,1]- test$Y
```

```{r}
plot(x = test_resp, y = mdl3_resids)
```

```{r}
plot(RMSEP(model3, newdata = test_scale))
```

```{r}
model4 <- pcr(Y~.,
               ncomp = 25,
               data = train_scale,
               validation = "CV")
```

```{r}
plot(RMSEP(model4))
plot(RMSEP(model4, newdata = test_scale))
```

## Linear Model With Highly Correlated Variables

```{r}
train_corr <- as.data.frame(apply(train_scale, 2, function(x){cor(x, y = train_scale$Y)}))
```

```{r}
train_corr <- train_corr[order(train_corr[,1],decreasing=TRUE),,drop = FALSE]
```

```{r}
lm_1 <- lm(Y~X10, train)
```

```{r}
library(caret)
cv_func <- function(x){
  train.control <- trainControl(method = "cv", number = 100)
  
  model <- train(Y~x, data = train, method = "lm",
               trControl = train.control)
}
```


```{r}
apply(cv_func, c(X10, X20, X6, X5, X11, X14, X19))
```


Figure out how to order while saving variable names



Try LASSO, Ridge, maybe some other techniques. Look at residual plots. Try with smaller groups of predictors, maybe most correlated.

Circumstances when methodology works best, use data more as example than focus.
